【算法优化思路】
一、【优化器】
1.SGD往往具有收敛速度慢的缺点，且极其依赖learning_rate这项超参数，我尝试采用自适应学习率优化算法，主要尝试了Adam和AdaDelta算法，前者对于超参数的选择较为鲁棒，而后者在模型训练的中期和初期会表现更好，提升训练速度，但在后期会出现反复或局部最小值附近抖动，这一点在训练相关参数的图像中也可以发现。
最终我选用了Adadelta算法作为优化器。
二、【网络架构】
1.首先利用torch.nn中的Sequational结构对网络架构的书写进行优化，提升代码的可重用性和可读性。
2.将Linear层增加到5层（包括最后起flatten作用的），交替relu层使用
3.由于本例中数据以矩阵形式输入，没有采用先复现为图片然后搭建卷积相关层进行处理的方式。
4.加入dropout层，提升曲线的平滑度。
三、【Dropout层的运用】
1.有关参数选取，由于本题我所采用的网络较浅，不适宜采用过高的P值（p=0
.5时可以发挥dropout最佳正则效果），经过调参确认p值为0.14。
2.在train_episode关闭dropout层，发现accuracy和value曲线会更加陡峭。
3.在AlexNet论文中Hinton大佬提出dropout是想用作“模型参数”的融合，在后续学者的研究发现，dropout层实际更像是一个L2正则项，使得模型收敛更加稳定，本作业完成后所绘图像也似乎间接证明了这一点。
四。【early stopping】
1.由于图像出现了提前反转的情况，所以其实可以将epoch训练轮数缩短到8epoch或9epoch